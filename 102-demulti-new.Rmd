---
title: "102-demulti-new"
author: "Mac Campbell"
date: "8/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

```{r}
library(tidyverse)
```

## Demulti new samples

Counting lines in the files to make sure demulti was complete. For example:
 awk '{l++} END {print l/4}' LibPlate25_MORedband02_R1.fastq
 
 l004:
gunzip -c BMAG072_S1_L004_R1_001.fastq.gz |  awk '{l++} END {print l/4}'
cat *R1.fastq | awk '{l++} END {print l/4}' 
These are 


We have three plates in two lanes. Due to space constraints, they are in two locations. 

Checking completeness:
`(base) maccamp@farm:/group/millermrgrp4/maccamp/alanas-trout-2$  gunzip -c BMAG072_S1_L004_R1_001.fastq.gz |  awk '{l++} END {print l/4}'`
2947604505    

__1__ Combine into one file     

Doing in data/combined-plates   

```{sh, eval=FALSE}
cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate21_LongfinSmelt01_R1.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate21_LongfinSmelt01_R1.fastq > longfin-01_R1.fastq &

cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate22_LongfinSmelt02_R1.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate22_LongfinSmelt02_R1.fastq > longfin-02_R1.fastq &

cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate23_LongfinSmelt03_R1.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate23_LongfinSmelt03_R1.fastq > longfin-03_R1.fastq &


cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate21_LongfinSmelt01_R2.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate21_LongfinSmelt01_R2.fastq > longfin-01_R2.fastq &

cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate22_LongfinSmelt02_R2.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate22_LongfinSmelt02_R2.fastq > longfin-02_R2.fastq &

cat /group/millermrgrp4/maccamp/alanas-trout/LibPlate23_LongfinSmelt03_R2.fastq          /group/millermrgrp4/maccamp/alanas-trout-2/LibPlate23_LongfinSmelt03_R2.fastq > longfin-03_R2.fastq &
```

__2__ demultiplex each plate
      Checking for barcodes
      grep GGACAAGCTATGCAGG --color longfin-03_R1.fastq (works)
      barcode is ACAAGCTA
      
      separate dirs in data/   
      Using column 2 identifier and column 9    
These are now in /group/millermrgrp4/maccamp/longfin/plate-*


```{sh, eval=FALSE}
    cat /home/maccamp/longfin/meta/sample-meta.tsv | grep "Plate 1" | awk '{ print " perl ./BarcodeSplitListBestRadPairedEnd.pl", "longfin-01_R1.fastq", "longfin-01_R2.fastq", "GG"$9"TGCAGG", $2 }' > tasks.sh  
             srun -p high -t 1-10:00:00 --nodes=1 parallel -j 12 < tasks.sh > std.out 2>std.err &   

      cat /home/maccamp/longfin/meta/sample-meta.tsv | grep "Plate 2" | awk '{ print " perl ./BarcodeSplitListBestRadPairedEnd.pl", "longfin-02_R1.fastq", "longfin-02_R2.fastq", "GG"$9"TGCAGG", $2 }' > tasks.sh  
           srun -p high -t 1-10:00:00 --nodes=1 parallel -j 12 < tasks.sh > std.out 2>std.err &   

     
      cat /home/maccamp/longfin/meta/sample-meta.tsv | grep "Plate 3" | awk '{ print " perl ./BarcodeSplitListBestRadPairedEnd.pl", "longfin-03_R1.fastq", "longfin-03_R2.fastq", "GG"$9"TGCAGG", $2 }' > tasks.sh  
      srun -p high -t 1-10:00:00 --nodes=1 parallel -j 12 < tasks.sh > std.out 2>std.err &   
```
      
      
## Alignment

Previously, I used the non-accessioned genome.

`(base) maccamp@farm:~/data/longfin$ samtools view -H ALVS_097_R1.sort.bam`
`@HD	VN:1.5	SO:coordinate`
`@SQ	SN:lg01	LN:12516727`
`@SQ	SN:lg02	LN:17206656`
`@SQ	SN:lg03	LN:15692960`
`bwa mem /home/maccamp/genomes/hypomesus-20210204/Hyp_tra_F_20210204.fa ALVS_097_R1.fastq.gz ALVS_097_R2.fastq.gz`

in each data/plate-x

ls | grep RA | perl -pe 's/.fastq//g' > forward 
ls | grep RB | perl -pe 's/.fastq//g' > reverse
ls | grep RA | cut -f 1-2 -d "_" > names
paste forward reverse names  > files.txt


bash /home/maccamp/longfin/scripts/doAlign-unzipped.sh files.txt /home/maccamp/genomes/hypomesus-20210204/Hyp_tra_F_20210204.fa

Summarizing counts for each plate.

```{sh, eval=FALSE}
ls | grep sort.flt.bam | grep -v bai | while read line; do samtools flagstat $line | grep mapped | head -n 1 >> counts.txt; done;
ls | grep sort.bam | grep -v bai | while read line; do samtools flagstat $line | grep mapped | head -n 1 >> counts-sort.txt; done;
ls | grep sort.flt.bam | grep -v bai >> counts.files.txt

 paste counts.files.txt counts-sort.txt counts.txt | perl -pe 's/\s\+\s0\smapped\s\(/\t/g' | perl -pe 's/% : N\/A\)//g' > plate1.txt

 paste counts.files.txt counts-sort.txt counts.txt | perl -pe 's/\s\+\s0\smapped\s\(/\t/g' | perl -pe 's/% : N\/A\)//g' > plate2.txt

 paste counts.files.txt counts-sort.txt counts.txt | perl -pe 's/\s\+\s0\smapped\s\(/\t/g' | perl -pe 's/% : N\/A\)//g' > plate3.txt

```

```{r}
readFiles<-function(file) {
  fdf<-read_tsv(file, col_names=c("Bam","AlignedReads","PercentAligned","DedupReads"))
  return(fdf)
}
```

```{r}
files<-list.files(path="outputs/102", pattern=".txt", full.names = TRUE)
fdfs<-bind_rows(readFiles(files)) %>% select(-X5)
fdfs$Identifier<-gsub(".sort.flt.bam","",fdfs$Bam)
fdfs
```

Bind meta     

```{r}
meta<-read_tsv("meta/sample-meta.tsv") %>% left_join(fdfs) 

meta
```

```{r}
meta %>% group_by(`Common Name`) %>% summarize(Count=n())
```

```{r}
ggplot(meta) +
  geom_histogram(aes(x=DedupReads,fill=`Common Name`)) +
  scale_fill_viridis_d(option = 'turbo') +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  ylab("Count") +
  xlab("Filtered Read Count")
```


```{r}
meta %>% group_by(`Common Name`) %>% summarize(Mean=mean(DedupReads))
```

```{r}
initial<-meta %>% filter(DedupReads > 150000)
initial %>% group_by(`Common Name`) %>% summarize(Count=n())
```

```{r}
ggplot(initial) +
  geom_histogram(aes(x=DedupReads,fill=`Common Name`)) +
  scale_fill_viridis_d(option = 'turbo') +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  ylab("Count") +
  xlab("Filtered Read Count")

```

Need to downsample, to 1.75M?

```{r}
downsize<-initial %>% filter(DedupReads>1.75e6) %>% select(Identifier, Bam) %>% mutate(Command=paste0("samtools view -bs $frac ",Bam," ",Identifier,".reduced.sort.flt.bam")) %>% select(Command)

write_tsv(downsize, "102.1-downsize-commands.sh", col_names = FALSE)
```


Previously I did something like this:  
Note: We have some high coverage reads we can downsample: https://davemcg.github.io/post/easy-bam-downsampling/ say to 1750000
frac=$( samtools idxstats input.bam | cut -f3 | awk ‘BEGIN {total=0} {total += $1} END {frac=1750000/total; if (frac > 1) {print 1} else {print frac}}’ )

samtools view -bs $frac input.bam > subsample.bam